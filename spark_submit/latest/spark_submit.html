
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Submiting spark</title>
<link href="../../_static/_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css" rel="stylesheet"/>
<link href="../../_static/_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../_static/_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../_static/_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../_static/_static/vendor/open-sans_all/1.44.1/index.css" rel="stylesheet"/>
<link href="../../_static/_static/vendor/lato_latin-ext/1.44.1/index.css" rel="stylesheet"/>
<link href="../../_static/_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../_static/_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" rel="stylesheet" type="text/css">
<link href="../../_static/_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../_static/_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../_static/_static/mystnb.css" rel="stylesheet" type="text/css">
<link as="script" href="../../_static/_static/js/index.d3f166471bb80abb5163.js" rel="preload"/>
<script data-url_root="./" id="documentation_options" src="../../_static/_static/documentation_options.js"></script>
<script src="../../_static/_static/jquery.js"></script>
<script src="../../_static/_static/underscore.js"></script>
<script src="../../_static/_static/doctools.js"></script>
<script src="../../_static/_static/clipboard.min.js"></script>
<script src="../../_static/_static/copybutton.js"></script>
<script src="../../_static/_static/togglebutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script src="../../_static/_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
<link href="genindex.html" rel="index" title="Index">
<link href="search.html" rel="search" title="Search">
<link href="sql_to_file.html" rel="next" title="SQL Ingest - Ingest data using SQL query"/>
<link href="slack_notify.html" rel="prev" title="mlconfig"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></link></link></link></head>
<body data-offset="80" data-spy="scroll" data-target="#bd-toc-nav">
<div class="container-xl">
<div class="row">
<div class="col-12 col-md-2 bd-sidebar site-navigation show single-page" id="site-navigation">
</div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
<div class="row topbar fixed-top container-xl">
<div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
</div>
<div class="col pl-2 topbar-main">
<div class="dropdown-buttons-trigger">
<button aria-label="Download this page" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fas fa-download"></i></button>
<div class="dropdown-buttons">
<!-- ipynb file if we had a myst markdown file -->
<!-- Download raw file -->
<a class="dropdown-buttons" href="_sources/spark_submit.ipynb.txt"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Download source file" type="button">.ipynb</button></a>
<!-- Download PDF via print -->
<button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" id="download-print" onclick="window.print()" title="Print to PDF" type="button">.pdf</button>
</div>
</div>
<!-- Source interaction buttons -->
<!-- Full screen (wrap in <a> to have style consistency -->
<a class="full-screen-button"><button aria-label="Fullscreen mode" class="btn btn-secondary topbarbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode" type="button"><i class="fas fa-expand"></i></button></a>
<!-- Launch buttons -->
</div>
<!-- Table of contents -->
<div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
        </div>
<nav id="bd-toc-nav">
<ul class="nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#function">
   Function
  </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#test">
   Test
  </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#define-the-execute-test-task">
     Define the execute test task
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#test-on-cluster">
     Test on cluster
    </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#show-results">
     Show results
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="row" id="main-content">
<div class="col-12 col-md-9 pl-md-3 pr-md-0">
<div>
<div class="section" id="submiting-spark">
<h1>Submiting spark<a class="headerlink" href="#submiting-spark" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="ch">#!pip install mlrun</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># nuclio: ignore</span>
<span class="kn">import</span> <span class="nn">nuclio</span>
</pre></div>
</div>
</div>
</div>
<p>Define the MLRun environment</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">nuclio</span> config kind = "job"
<span class="o">%</span><span class="k">nuclio</span> config spec.image = "mlrun/mlrun"
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>%nuclio: setting kind to 'job'
%nuclio: setting spec.image to 'mlrun/mlrun'
</pre></div>
</div>
</div>
</div>
<div class="section" id="function">
<h2>Function<a class="headerlink" href="#function" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlrun</span> <span class="kn">import</span> <span class="n">get_or_create_ctx</span>
<span class="kn">from</span> <span class="nn">kubernetes</span> <span class="kn">import</span> <span class="n">config</span><span class="p">,</span> <span class="n">client</span>
<span class="kn">from</span> <span class="nn">kubernetes.stream</span> <span class="kn">import</span> <span class="n">stream</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">K8SClient</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logger</span><span class="p">,</span> <span class="n">namespace</span><span class="o">=</span><span class="s1">'default-tenant'</span><span class="p">,</span> <span class="n">config_file</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">namespace</span> <span class="o">=</span> <span class="n">namespace</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logger</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_k8s_config</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v1api</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">CoreV1Api</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init_k8s_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_file</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">load_incluster_config</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">'using in-cluster config.'</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">config</span><span class="o">.</span><span class="n">load_kube_config</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">'using local kubernetes config.'</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s1">'cannot find local kubernetes config file,'</span>
                    <span class="s1">' place it in ~/.kube/config or specify it in '</span>
                    <span class="s1">'KUBECONFIG env var'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_shell_pod_name</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pod_name</span><span class="o">=</span><span class="s1">'shell'</span><span class="p">):</span>
        <span class="n">shell_pod</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v1api</span><span class="o">.</span><span class="n">list_namespaced_pod</span><span class="p">(</span><span class="n">namespace</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">namespace</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">shell_pod</span><span class="o">.</span><span class="n">items</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">pod_name</span> <span class="ow">in</span> <span class="n">i</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"</span><span class="si">%s</span><span class="se">\t</span><span class="si">%s</span><span class="se">\t</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">status</span><span class="o">.</span><span class="n">pod_ip</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">namespace</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
                <span class="n">shell_name</span> <span class="o">=</span> <span class="n">i</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">name</span>
                <span class="k">break</span>
        <span class="k">return</span> <span class="n">shell_name</span>

    <span class="k">def</span> <span class="nf">exec_shell_cmd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cmd</span><span class="p">,</span> <span class="n">shell_pod_name</span> <span class="o">=</span> <span class="s1">'shell'</span><span class="p">):</span>
        <span class="n">shell_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_shell_pod_name</span><span class="p">(</span><span class="n">shell_pod_name</span><span class="p">)</span>
        <span class="c1"># Calling exec and waiting for response</span>
        <span class="n">exec_command</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s1">'/bin/bash'</span><span class="p">,</span>
            <span class="s1">'-c'</span><span class="p">,</span>
            <span class="n">cmd</span><span class="p">]</span>
        <span class="n">resp</span> <span class="o">=</span> <span class="n">stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v1api</span><span class="o">.</span><span class="n">connect_get_namespaced_pod_exec</span><span class="p">,</span>
                      <span class="n">shell_name</span><span class="p">,</span>
                      <span class="bp">self</span><span class="o">.</span><span class="n">namespace</span><span class="p">,</span>
                      <span class="n">command</span><span class="o">=</span><span class="n">exec_command</span><span class="p">,</span>
                      <span class="n">stderr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stdin</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                      <span class="n">stdout</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tty</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Response: "</span> <span class="o">+</span> <span class="n">resp</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">spark_command_builder</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">class_name</span><span class="p">,</span> <span class="n">jars</span><span class="p">,</span> <span class="n">packages</span><span class="p">,</span> <span class="n">spark_options</span><span class="p">):</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="s1">'spark-submit'</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">:</span>
        <span class="n">cmd</span> <span class="o">+=</span> <span class="s1">' --name '</span> <span class="o">+</span> <span class="n">name</span>

    <span class="k">if</span> <span class="n">class_name</span><span class="p">:</span>
        <span class="n">cmd</span> <span class="o">+=</span> <span class="s1">' --class '</span> <span class="o">+</span> <span class="n">class_name</span>

    <span class="k">if</span> <span class="n">jars</span><span class="p">:</span>
        <span class="n">cmd</span> <span class="o">+=</span> <span class="s1">' --jars '</span> <span class="o">+</span> <span class="n">jars</span>

    <span class="k">if</span> <span class="n">packages</span><span class="p">:</span>
        <span class="n">cmd</span> <span class="o">+=</span> <span class="s1">' --packages '</span> <span class="o">+</span> <span class="n">packages</span>

    <span class="k">if</span> <span class="n">spark_options</span><span class="p">:</span>
        <span class="n">cmd</span> <span class="o">+=</span> <span class="s1">' '</span> <span class="o">+</span> <span class="n">spark_options</span>

    <span class="k">return</span> <span class="n">cmd</span>


<span class="k">def</span> <span class="nf">spark_submit</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">v3io_access_key</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">class_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">jars</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">packages</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">spark_options</span><span class="o">=</span><span class="s1">''</span><span class="p">):</span>
    <span class="sd">"""spark_submit function</span>
<span class="sd">    </span>
<span class="sd">    submiting spark via shell</span>
<span class="sd">    </span>
<span class="sd">    :param name:        A name of your application.</span>
<span class="sd">    :param class_name:  Your application's main class (for Java / Scala apps).</span>
<span class="sd">                        * If relative will add to the {artifact_path}</span>
<span class="sd">    :param jars:        Comma-separated list of jars to include on the driver</span>
<span class="sd">                        and executor classpaths.</span>
<span class="sd">    :param packages:    Comma-separated list of maven coordinates of jars to include</span>
<span class="sd">                        on the driver and executor classpaths. Will search the local</span>
<span class="sd">                        maven repo, then maven central and any additional remote</span>
<span class="sd">                        repositories given by --repositories. The format for the</span>
<span class="sd">    :param spark_options: spark parametes that are not included as function arguments</span>
<span class="sd">    """</span>
    <span class="n">cmd</span> <span class="o">=</span> <span class="n">spark_command_builder</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">class_name</span><span class="p">,</span> <span class="n">jars</span><span class="p">,</span> <span class="n">packages</span><span class="p">,</span> <span class="n">spark_options</span><span class="p">)</span>
    <span class="n">context</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"submiting :"</span> <span class="o">+</span> <span class="n">cmd</span><span class="p">)</span>
    <span class="n">cli</span> <span class="o">=</span> <span class="n">K8SClient</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">logger</span><span class="p">)</span>
    <span class="n">cli</span><span class="o">.</span><span class="n">exec_shell_cmd</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># nuclio: end-code</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="test">
<h2>Test<a class="headerlink" href="#test" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>This test uses the metrics data, created by the <a class="reference external" href="https://github.com/mlrun/demo-network-operations/blob/master/notebooks/generator.ipynb">Generator function</a> from MLRun’s <a class="reference external" href="https://github.com/mlrun/demo-network-operations">Network Operations Demo</a><br/>
To test it yourself, please generate this dataset or use any of your available csv/parquet datasets.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">mlrun</span> <span class="kn">import</span> <span class="n">code_to_function</span><span class="p">,</span> <span class="n">mount_v3io</span><span class="p">,</span> <span class="n">NewTask</span><span class="p">,</span> <span class="n">mlconf</span><span class="p">,</span> <span class="n">run_local</span>
<span class="n">mlconf</span><span class="o">.</span><span class="n">dbpath</span> <span class="o">=</span> <span class="n">mlconf</span><span class="o">.</span><span class="n">dbpath</span> <span class="ow">or</span> <span class="s1">'http://mlrun-api:8080'</span>
<span class="n">mlconf</span><span class="o">.</span><span class="n">artifact_path</span> <span class="o">=</span> <span class="n">mlconf</span><span class="o">.</span><span class="n">artifact_path</span> <span class="ow">or</span> <span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"HOME"</span><span class="p">]</span><span class="si">}</span><span class="s1">/artifacts'</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="define-the-execute-test-task">
<h3>Define the execute test task<a class="headerlink" href="#define-the-execute-test-task" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">execute_task</span> <span class="o">=</span> <span class="n">NewTask</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">'spark-submit'</span><span class="p">,</span>
                         <span class="n">project</span><span class="o">=</span><span class="s1">'submit-proj'</span><span class="p">,</span>
                         <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s1">'spark_options'</span><span class="p">:</span><span class="s2">"/spark/examples/jars/spark-examples_2.11-2.4.4.jar 10"</span><span class="p">,</span><span class="s1">'class_name'</span><span class="p">:</span><span class="s1">'org.apache.spark.examples.SparkPi'</span><span class="p">},</span>                          
                         <span class="n">handler</span><span class="o">=</span><span class="n">spark_submit</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#submit_run = run_local(submit_task)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="test-on-cluster">
<h3>Test on cluster<a class="headerlink" href="#test-on-cluster" title="Permalink to this headline">¶</a></h3>
<p>Convert the code to an MLRun function</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fn</span> <span class="o">=</span> <span class="n">code_to_function</span><span class="p">(</span><span class="s1">'spark-submit'</span><span class="p">,</span> <span class="n">handler</span><span class="o">=</span><span class="s1">'spark_submit'</span><span class="p">)</span>
<span class="n">fn</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">service_account</span><span class="o">=</span><span class="s1">'mlrun-api'</span>
<span class="n">fn</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">mount_v3io</span><span class="p">())</span>
<span class="n">fn</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="s1">'function.yaml'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[mlrun] 2020-06-17 14:47:58,078 function spec saved to path: function.yaml
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;mlrun.runtimes.kubejob.KubejobRuntime at 0x7f69a4117c50&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">execute_run</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">execute_task</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[mlrun] 2020-06-17 14:47:58,101 warning!, server (0.4.7) and client (0.4.9) ver dont match
[mlrun] 2020-06-17 14:47:58,101 starting run spark-submit uid=27391b5c221e4409baf4ae896169768b  -&gt; http://mlrun-api:8080
[mlrun] 2020-06-17 14:47:58,204 Job is running in the background, pod: spark-submit-wrc48
[mlrun] 2020-06-17 14:48:02,407 starting local run: main.py # spark_submit
[mlrun] 2020-06-17 14:48:02,423 submiting :spark-submit --class org.apache.spark.examples.SparkPi /spark/examples/jars/spark-examples_2.11-2.4.4.jar 10
[mlrun] 2020-06-17 14:48:02,423 using in-cluster config.
[mlrun] 2020-06-17 14:48:02,516 10.200.0.53	default-tenant	shell-658dd9bb8f-8q5sr
[mlrun] 2020-06-17 14:48:08,739 Response: 20/06/17 14:48:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/06/17 14:48:04 INFO spark.SparkContext: Running Spark version 2.4.4
20/06/17 14:48:04 INFO spark.SparkContext: Submitted application: Spark Pi
20/06/17 14:48:04 INFO spark.SecurityManager: Changing view acls to: iguazio
20/06/17 14:48:04 INFO spark.SecurityManager: Changing modify acls to: iguazio
20/06/17 14:48:04 INFO spark.SecurityManager: Changing view acls groups to: 
20/06/17 14:48:04 INFO spark.SecurityManager: Changing modify acls groups to: 
20/06/17 14:48:04 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(iguazio); groups with view permissions: Set(); users  with modify permissions: Set(iguazio); groups with modify permissions: Set()
20/06/17 14:48:04 INFO util.Utils: Successfully started service 'sparkDriver' on port 34054.
20/06/17 14:48:04 INFO spark.SparkEnv: Registering MapOutputTracker
20/06/17 14:48:04 INFO spark.SparkEnv: Registering BlockManagerMaster
20/06/17 14:48:04 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/06/17 14:48:04 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/06/17 14:48:04 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-b5993354-454e-47e8-b317-349b4268fe6a
20/06/17 14:48:04 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
20/06/17 14:48:04 INFO spark.SparkEnv: Registering OutputCommitCoordinator
20/06/17 14:48:04 INFO util.log: Logging initialized @1657ms
20/06/17 14:48:04 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
20/06/17 14:48:04 INFO server.Server: Started @1718ms
20/06/17 14:48:04 INFO server.AbstractConnector: Started ServerConnector@7e97551f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20/06/17 14:48:04 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1450078a{/jobs,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@534ca02b{/jobs/json,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a23c3d{/jobs/job,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fe46b62{/jobs/job/json,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591fd34d{/stages,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61e45f87{/stages/json,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c9b78e3{/stages/stage,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5491f68b{/stages/stage/json,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@736ac09a{/stages/pool,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ecd665{/stages/pool/json,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45394b31{/storage,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ec7d8b3{/storage/json,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b0ca5e1{/storage/rdd,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bb3131b{/storage/rdd/json,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54dcbb9f{/environment,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fef3f7{/environment/json,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a037324{/executors,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69eb86b4{/executors/json,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@585ac855{/executors/threadDump,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bb8f9e2{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a933be2{/static,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@378bd86d{/,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2189e7a7{/api,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@644abb8f{/jobs/job/kill,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a411233{/stages/stage/kill,null,AVAILABLE,@Spark}
20/06/17 14:48:04 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.200.0.53:4040
20/06/17 14:48:04 INFO spark.SparkContext: Added JAR file:///spark/v3io-libs/v3io-hcfs_2.11.jar at spark://10.200.0.53:34054/jars/v3io-hcfs_2.11.jar with timestamp 1592405284608
20/06/17 14:48:04 INFO spark.SparkContext: Added JAR file:///spark/v3io-libs/v3io-spark2-object-dataframe_2.11.jar at spark://10.200.0.53:34054/jars/v3io-spark2-object-dataframe_2.11.jar with timestamp 1592405284609
20/06/17 14:48:04 INFO spark.SparkContext: Added JAR file:///spark/v3io-libs/v3io-spark2-streaming_2.11.jar at spark://10.200.0.53:34054/jars/v3io-spark2-streaming_2.11.jar with timestamp 1592405284609
20/06/17 14:48:04 INFO spark.SparkContext: Added JAR file:/spark/examples/jars/spark-examples_2.11-2.4.4.jar at spark://10.200.0.53:34054/jars/spark-examples_2.11-2.4.4.jar with timestamp 1592405284609
20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
20/06/17 14:48:04 INFO client.TransportClientFactory: Successfully created connection to spark-master/10.194.174.4:7077 after 29 ms (0 ms spent in bootstraps)
20/06/17 14:48:04 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20200617144804-0002
20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200617144804-0002/0 on worker-20200617131947-10.200.0.51-38368 (10.200.0.51:38368) with 1 core(s)
20/06/17 14:48:04 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200617144804-0002/0 on hostPort 10.200.0.51:38368 with 1 core(s), 2.0 GB RAM
20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200617144804-0002/1 on worker-20200617131947-10.200.0.51-38368 (10.200.0.51:38368) with 1 core(s)
20/06/17 14:48:04 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200617144804-0002/1 on hostPort 10.200.0.51:38368 with 1 core(s), 2.0 GB RAM
20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200617144804-0002/2 on worker-20200617131947-10.200.0.51-38368 (10.200.0.51:38368) with 1 core(s)
20/06/17 14:48:04 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200617144804-0002/2 on hostPort 10.200.0.51:38368 with 1 core(s), 2.0 GB RAM
20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20200617144804-0002/3 on worker-20200617131947-10.200.0.51-38368 (10.200.0.51:38368) with 1 core(s)
20/06/17 14:48:04 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20200617144804-0002/3 on hostPort 10.200.0.51:38368 with 1 core(s), 2.0 GB RAM
20/06/17 14:48:04 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45909.
20/06/17 14:48:04 INFO netty.NettyBlockTransferService: Server created on 10.200.0.53:45909
20/06/17 14:48:04 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200617144804-0002/0 is now RUNNING
20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200617144804-0002/1 is now RUNNING
20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200617144804-0002/2 is now RUNNING
20/06/17 14:48:04 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20200617144804-0002/3 is now RUNNING
20/06/17 14:48:04 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.200.0.53, 45909, None)
20/06/17 14:48:04 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.53:45909 with 366.3 MB RAM, BlockManagerId(driver, 10.200.0.53, 45909, None)
20/06/17 14:48:04 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.200.0.53, 45909, None)
20/06/17 14:48:04 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.200.0.53, 45909, None)
20/06/17 14:48:05 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67440de6{/metrics/json,null,AVAILABLE,@Spark}
20/06/17 14:48:05 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
20/06/17 14:48:05 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:38
20/06/17 14:48:05 INFO scheduler.DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions
20/06/17 14:48:05 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)
20/06/17 14:48:05 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/06/17 14:48:05 INFO scheduler.DAGScheduler: Missing parents: List()
20/06/17 14:48:05 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents
20/06/17 14:48:05 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1936.0 B, free 366.3 MB)
20/06/17 14:48:05 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1256.0 B, free 366.3 MB)
20/06/17 14:48:05 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.53:45909 (size: 1256.0 B, free: 366.3 MB)
20/06/17 14:48:05 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
20/06/17 14:48:05 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))
20/06/17 14:48:05 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 10 tasks
20/06/17 14:48:07 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.51:46482) with ID 1
20/06/17 14:48:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.200.0.51, executor 1, partition 0, PROCESS_LOCAL, 7870 bytes)
20/06/17 14:48:07 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.51:46490) with ID 0
20/06/17 14:48:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.200.0.51, executor 0, partition 1, PROCESS_LOCAL, 7870 bytes)
20/06/17 14:48:07 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.51:44195 with 912.3 MB RAM, BlockManagerId(1, 10.200.0.51, 44195, None)
20/06/17 14:48:07 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.51:46498) with ID 2
20/06/17 14:48:07 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 10.200.0.51, executor 2, partition 2, PROCESS_LOCAL, 7870 bytes)
20/06/17 14:48:07 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.200.0.51:46504) with ID 3
20/06/17 14:48:07 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, 10.200.0.51, executor 3, partition 3, PROCESS_LOCAL, 7870 bytes)
20/06/17 14:48:07 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.51:45765 with 912.3 MB RAM, BlockManagerId(0, 10.200.0.51, 45765, None)
20/06/17 14:48:07 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.51:38749 with 912.3 MB RAM, BlockManagerId(2, 10.200.0.51, 38749, None)
20/06/17 14:48:07 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.200.0.51:37838 with 912.3 MB RAM, BlockManagerId(3, 10.200.0.51, 37838, None)
20/06/17 14:48:07 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.51:44195 (size: 1256.0 B, free: 912.3 MB)
20/06/17 14:48:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.51:45765 (size: 1256.0 B, free: 912.3 MB)
20/06/17 14:48:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.51:37838 (size: 1256.0 B, free: 912.3 MB)
20/06/17 14:48:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.200.0.51:38749 (size: 1256.0 B, free: 912.3 MB)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, 10.200.0.51, executor 1, partition 4, PROCESS_LOCAL, 7870 bytes)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1092 ms on 10.200.0.51 (executor 1) (1/10)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, 10.200.0.51, executor 0, partition 5, PROCESS_LOCAL, 7870 bytes)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1028 ms on 10.200.0.51 (executor 0) (2/10)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, 10.200.0.51, executor 1, partition 6, PROCESS_LOCAL, 7870 bytes)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, 10.200.0.51, executor 0, partition 7, PROCESS_LOCAL, 7870 bytes)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 57 ms on 10.200.0.51 (executor 0) (3/10)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 76 ms on 10.200.0.51 (executor 1) (4/10)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, 10.200.0.51, executor 3, partition 8, PROCESS_LOCAL, 7870 bytes)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, 10.200.0.51, executor 0, partition 9, PROCESS_LOCAL, 7870 bytes)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 938 ms on 10.200.0.51 (executor 3) (5/10)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 32 ms on 10.200.0.51 (executor 0) (6/10)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 46 ms on 10.200.0.51 (executor 1) (7/10)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 21 ms on 10.200.0.51 (executor 0) (8/10)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 973 ms on 10.200.0.51 (executor 2) (9/10)
20/06/17 14:48:08 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 30 ms on 10.200.0.51 (executor 3) (10/10)
20/06/17 14:48:08 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/06/17 14:48:08 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 2.688 s
20/06/17 14:48:08 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 2.754248 s
Pi is roughly 3.1406551406551406
20/06/17 14:48:08 INFO server.AbstractConnector: Stopped Spark@7e97551f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20/06/17 14:48:08 INFO ui.SparkUI: Stopped Spark web UI at http://10.200.0.53:4040
20/06/17 14:48:08 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
20/06/17 14:48:08 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
20/06/17 14:48:08 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/06/17 14:48:08 INFO memory.MemoryStore: MemoryStore cleared
20/06/17 14:48:08 INFO storage.BlockManager: BlockManager stopped
20/06/17 14:48:08 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
20/06/17 14:48:08 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/06/17 14:48:08 INFO spark.SparkContext: Successfully stopped SparkContext
20/06/17 14:48:08 INFO util.ShutdownHookManager: Shutdown hook called
20/06/17 14:48:08 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f4b281e0-b737-4f49-bb84-45cc1eeffcbc
20/06/17 14:48:08 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1f552705-8218-4bf4-9f8b-90afe47c4638


[mlrun] 2020-06-17 14:48:08,752 run executed, status=completed
final state: succeeded
</pre></div>
</div>
<div class="output text_html"><style> 
.dictlist {
  background-color: #b3edff; 
  text-align: center; 
  margin: 4px; 
  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}
.artifact {
  cursor: pointer; 
  background-color: #ffe6cc; 
  text-align: left; 
  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;
}
div.block.hidden {
  display: none;
}
.clickable {
  cursor: pointer;
}
.ellipsis {
  display: inline-block;
  max-width: 60px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}
.master-wrapper {
  display: flex;
  flex-flow: row nowrap;
  justify-content: flex-start;
  align-items: stretch;
}
.master-tbl {
  flex: 3
}
.master-wrapper > div {
  margin: 4px;
  padding: 10px;
}
iframe.fileview {
  border: 0 none;
  height: 100%;
  width: 100%;
  white-space: pre-wrap;
}
.pane-header-title {
  width: 80%;
  font-weight: 500;
}
.pane-header {
  line-height: 1;
  background-color: #ffe6cc;
  padding: 3px;
}
.pane-header .close {
  font-size: 20px;
  font-weight: 700;
  float: right;
  margin-top: -5px;
}
.master-wrapper .right-pane {
  border: 1px inset silver;
  width: 40%;
  min-height: 300px;
  flex: 3
  min-width: 500px;
}
.master-wrapper * {
  box-sizing: border-box;
}
</style><script>
function copyToClipboard(fld) {
    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {
        var textarea = document.createElement('textarea');
        textarea.textContent = fld.innerHTML;
        textarea.style.position = 'fixed';
        document.body.appendChild(textarea);
        textarea.select();

        try {
            return document.execCommand('copy'); // Security exception may be thrown by some browsers.
        } catch (ex) {

        } finally {
            document.body.removeChild(textarea);
        }
    }
}
function expandPanel(el) {
  const panelName = "#" + el.getAttribute('paneName');
  console.log(el.title);

  document.querySelector(panelName + "-title").innerHTML = el.title
  iframe = document.querySelector(panelName + "-body");

  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}
    #csv { margin-bottom: 15px; }
    #csv table { border-collapse: collapse;}
    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;

  function csvToHtmlTable(str) {
    return '<div id="csv"><table><tr><td>' +  str.replace(/[\n\r]+$/g, '').replace(/[\n\r]+/g, '</td></tr><tr><td>')
      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';
  }

  function reqListener () {
    if (el.title.endsWith(".csv")) {
      iframe.setAttribute("srcdoc", tblcss + csvToHtmlTable(this.responseText));
    } else {
      iframe.setAttribute("srcdoc", this.responseText);
    }  
    console.log(this.responseText);
  }

  const oReq = new XMLHttpRequest();
  oReq.addEventListener("load", reqListener);
  oReq.open("GET", el.title);
  oReq.send();


  //iframe.src = el.title;
  const resultPane = document.querySelector(panelName + "-pane");
  if (resultPane.classList.contains("hidden")) {
    resultPane.classList.remove("hidden");
  }
}
function closePanel(el) {
  const panelName = "#" + el.getAttribute('paneName')
  const resultPane = document.querySelector(panelName + "-pane");
  if (!resultPane.classList.contains("hidden")) {
    resultPane.classList.add("hidden");
  }
}

</script>
<div class="master-wrapper">
<div class="block master-tbl"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>project</th>
<th>uid</th>
<th>iter</th>
<th>start</th>
<th>state</th>
<th>name</th>
<th>labels</th>
<th>inputs</th>
<th>parameters</th>
<th>results</th>
<th>artifacts</th>
</tr>
</thead>
<tbody>
<tr>
<td>submit-proj</td>
<td><div title="27391b5c221e4409baf4ae896169768b"><a href="https://mlrun-ui.default-tenant.app.app-lab-dev.iguazio-cd0.com/projects/submit-proj/jobs/27391b5c221e4409baf4ae896169768b/info" target="_blank">...6169768b</a></div></td>
<td>0</td>
<td>Jun 17 14:48:02</td>
<td>completed</td>
<td>spark-submit</td>
<td><div class="dictlist">host=spark-submit-wrc48</div><div class="dictlist">kind=job</div><div class="dictlist">owner=admin</div><div class="dictlist">v3io_user=admin</div></td>
<td></td>
<td><div class="dictlist">class_name=org.apache.spark.examples.SparkPi</div><div class="dictlist">spark_options=/spark/examples/jars/spark-examples_2.11-2.4.4.jar 10</div></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div></div>
<div class="right-pane block hidden" id="result3a99abc3-pane">
<div class="pane-header">
<span class="pane-header-title" id="result3a99abc3-title">Title</span>
<span class="close clickable" onclick="closePanel(this)" panename="result3a99abc3">×</span>
</div>
<iframe class="fileview" id="result3a99abc3-body"></iframe>
</div>
</div>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>to track results use .show() or .logs() or in CLI: 
!mlrun get run 27391b5c221e4409baf4ae896169768b --project submit-proj , !mlrun logs 27391b5c221e4409baf4ae896169768b --project submit-proj
[mlrun] 2020-06-17 14:48:10,433 run executed, status=completed
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="show-results">
<h3>Show results<a class="headerlink" href="#show-results" title="Permalink to this headline">¶</a></h3>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="prev-next-bottom">
<a class="left-prev" href="slack_notify.html" id="prev-link" title="previous page">mlconfig</a>
<a class="right-next" href="sql_to_file.html" id="next-link" title="next page">SQL Ingest - Ingest data using SQL query</a>
</div>
<footer class="footer mt-5 mt-md-0">
<div class="container">
<p>
        
            © Copyright .<br/>
</p>
</div>
</footer>
</main>
</div>
</div>
<script src="../../_static/_static/js/index.d3f166471bb80abb5163.js"></script>
</body>
</html>